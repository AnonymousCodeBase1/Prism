{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[490, 1460, 393, 409, 459, 248, 401, 517, 181, 126, 443, 498, 178, 168, 183, 177, 169, 167, 179, 170]\n",
      "[93, 31, 216, 219, 163, 220, 761, 53, 56, 42, 38, 465, 691, 217, 389, 249, 87, 84, 491, 786]\n",
      "[20, 901, 1013, 358, 69, 200, 949, 168, 53, 1460, 196, 75, 205, 36, 1237, 1253, 49, 7, 65, 1221]\n",
      "[35, 31, 51, 517, 47, 82, 60, 71, 40, 37, 39, 32, 43, 38, 72, 15, 44, 41, 8, 4]\n",
      "[1, 140, 44, 152, 38, 517, 45, 30, 46, 42, 34, 214, 200, 136, 87, 31, 43, 213, 28, 219]\n",
      "[1460, 353, 35, 45, 422, 6, 31, 37, 473, 1350, 41, 946, 52, 64, 171, 53, 57, 190, 506, 302]\n",
      "[38, 46, 286, 377, 1027, 1, 285, 190, 2882, 370, 376, 517, 1024, 287, 126, 141, 207, 1020, 1036, 142]\n",
      "[459, 91, 194, 165, 181, 507, 197, 506, 508, 166, 245, 509, 150, 213, 196, 264, 421, 164, 188, 193]\n",
      "[517, 93, 38, 99, 85, 126, 80, 78, 35, 51, 89, 128, 86, 84, 43, 96, 625, 69, 87, 90]\n",
      "[1380, 441, 439, 1460, 67, 181, 1029, 1006, 93, 31, 1019, 126, 35, 1, 1447, 517, 51, 191, 1351, 1415]\n",
      "[1460, 16413, 376, 344, 80, 68, 5840, 29200, 27740, 7300, 2172, 30660, 11680, 4380, 13140, 10220, 3632, 2920, 8760, 32832]\n",
      "[20, 1350, 13, 4, 69, 1, 15, 8, 19, 166, 38, 34, 214, 75, 310, 14, 342, 33, 197, 51]\n",
      "[133, 151, 615, 217, 1, 203, 303, 8, 290, 320, 319, 571, 316, 284, 289, 140, 285, 230, 635, 332]\n",
      "[373, 2676, 46, 1338, 344, 42, 53, 50, 38, 108, 167, 309, 517, 214, 150, 166, 72, 120, 262, 117]\n",
      "[262, 263, 276, 249, 287, 239, 307, 283, 126, 279, 308, 1, 313, 240, 227, 251, 93, 517, 259, 752]\n",
      "[517, 51, 1338, 126, 464, 46, 42, 85, 976, 1092, 463, 53, 50, 38, 262, 245, 1183, 208, 462, 571]\n",
      "[1350, 190, 506, 435, 302, 118, 217, 275, 38, 87, 90, 219, 1057, 225, 68, 218, 30, 97, 148, 223]\n",
      "[85, 36, 86, 35, 87, 517, 132, 131, 84, 133, 642, 134, 318, 80, 93, 94, 83, 92, 96, 398]\n",
      "[1452, 1412, 1440, 517, 85, 81, 36, 86, 1, 126, 46, 99, 42, 38, 93, 351, 412, 323, 262, 95]\n",
      "[517, 93, 1, 68, 98, 99, 51, 696, 97, 1107, 36, 92, 43, 38, 708, 555, 96, 31, 94, 80]\n"
     ]
    }
   ],
   "source": [
    "import os,shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class save_data(object):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "with open(\"merge_data.pkl\",\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "#dir, pktlen, payload len, flag, label\n",
    "data_dir = {}\n",
    "\n",
    "for item in data.data:\n",
    "\n",
    "    label = item[0][-1]\n",
    "    if label not in list(data_dir.keys()):\n",
    "        data_dir[label] ={}\n",
    "    \n",
    "    for pkt in item:\n",
    "        if pkt[0] == -1:\n",
    "            continue\n",
    "        lens = pkt[2] \n",
    "        if lens == 0:\n",
    "            continue\n",
    "        \n",
    "        if lens not in list(data_dir[label].keys()):\n",
    "            data_dir[label][lens] = 0\n",
    "        data_dir[label][lens] += 1\n",
    "        \n",
    "data_dir_tuple = {}\n",
    "\n",
    "for label in data_dir.keys():\n",
    "    data_dir_tuple[label] = sorted(data_dir[label].items(),key=lambda item:item[1],reverse = True)\n",
    "    \n",
    "top20_dir = {}\n",
    "for label in data_dir_tuple.keys():\n",
    "\n",
    "    top20_dir[label] = [item[0] for item in data_dir_tuple[label][:20]]\n",
    "    \n",
    "data_dir_adj = {}\n",
    "for flow in data.data:\n",
    "    label = flow[0][-1]\n",
    "    \n",
    "    if label not in list(data_dir_adj.keys()):\n",
    "        data_dir_adj[label] = []\n",
    "    tmp_flow = [item[1] for item in flow]\n",
    "    adj_flow = []\n",
    "    for item in tmp_flow:\n",
    "        idx_val = [abs(item-cand) for cand in top20_dir[label]]\n",
    "        adj_flow.append(top20_dir[label][np.argmin(np.array(idx_val))])\n",
    "    data_dir_adj[label].append(adj_flow)\n",
    "\n",
    "\n",
    "#define a trans seq of 20  top20_dir data_dir_adj\n",
    "trans_dir_20 = {}\n",
    "seq_len = 2\n",
    "for label in data_dir_adj:\n",
    "    if label not in list(trans_dir_20.keys()):\n",
    "        trans_dir_20[label] = np.zeros((seq_len,20,20))\n",
    "    top_20 = [item for item in top20_dir[label] ]\n",
    "    #initial point\n",
    "    for flow in data_dir_adj[label]:\n",
    "#         for i in range(seq_len):\n",
    "#             if i >len(flow)-1:\n",
    "#                 break\n",
    "        idx = top_20.index(flow[0])\n",
    "        trans_dir_20[label][0][idx][idx] += 1\n",
    "    \n",
    "#     trans_dir_20[label][0] = trans_dir_20[label][0]/np.diagonal(trans_dir_20[label][0]).sum()\n",
    "    #trans matrix\n",
    "    # split trans pairs\n",
    "    print(top_20)\n",
    "    for i in range(len(data_dir_adj[label])):\n",
    "        flow = data_dir_adj[label][i]\n",
    "        for j in range(1,len(flow)):\n",
    "#             print(flow[j])\n",
    "            trans_dir_20[label][1][top_20.index(flow[j-1])][top_20.index(flow[j])] += 1\n",
    "#             print(top_20.index(flow[j-1]),top_20.index(flow[j]))\n",
    "    \n",
    "        \n",
    "        \n",
    "#dir, pktlen, payload len, flag, label\n",
    "data_len_dir = {}\n",
    "\n",
    "for item in data.data:\n",
    "\n",
    "    label = item[0][-1]\n",
    "    if label not in list(data_len_dir.keys()):\n",
    "        data_len_dir[label] =[]\n",
    "    \n",
    "    data_len_dir[label].append(item)\n",
    "\n",
    "    \n",
    "def randomSeq(maxValue, num):\n",
    "\n",
    "    maxValue = int(maxValue)\n",
    "    suiji_ser = random.sample(range(1,maxValue), k=num-1) # 在1~maxValue之间，采集20个数据\n",
    "    suiji_ser.append(0)   # 加上数据开头\n",
    "    suiji_ser.append(maxValue)\n",
    "    suiji_ser = sorted(suiji_ser)\n",
    "    per_all_persons = [ suiji_ser[i]-suiji_ser[i-1] for i in range(1, len(suiji_ser)) ] # 列表推导式，计算列表中每两个数之间的间隔\n",
    "    \n",
    "    return per_all_persons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handshake seq counting\n",
    "handshake_seq_dir = {}\n",
    "for label in list(data_len_dir.keys()):\n",
    "    tmp = {}\n",
    "    for flow in data_len_dir[label]:\n",
    "#         print(item)\n",
    "        if flow[0][-3] not in list(tmp.keys()):\n",
    "            tmp[flow[0][-3]] = 0\n",
    "        tmp[flow[0][-3]] += 1\n",
    "    \n",
    "    tmp = sorted(tmp.items(), key=lambda d:d[1], reverse = True)\n",
    "    handshake_seq_dir[label] = tmp[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enhanced Targeted attacking, two items should be provided : trans_array => trans_dir_20, adjusted length(headers) => top20_dir\n",
    "import random,math\n",
    "rej = ['FTPS','SFTP']\n",
    "selected = ['360', 'aiqiyi', 'baidupan', 'huya', 'mgtv']\n",
    "targeted_flows_dir_dir = {}#{src_clss0: {target_clss0: [], target_clss1\" [] ....}, src_clss1: {target_clss0: [], target_clss1\" [] ....} ...}\n",
    "for src_clss in list(handshake_seq_dir.keys()):\n",
    "    if src_clss in rej:\n",
    "        continue\n",
    "#     if src_clss not in selected:\n",
    "#         continue\n",
    "    targeted_flows_dir_dir[src_clss] = {}\n",
    "    targeted_flows_dir = {} # {src_clss0: {target_clss0: [], target_clss1\" [] ....}\n",
    "    for target_clss in list(handshake_seq_dir.keys()):\n",
    "        if src_clss == target_clss or target_clss in rej:\n",
    "            continue\n",
    "#         if target_clss not in selected:\n",
    "#             continue\n",
    "        targeted_flows_dir[target_clss] = []\n",
    "        print(\"***** Refracting \", src_clss,\" ==> \",target_clss, \" *****\")\n",
    "        # src_clss = '360'\n",
    "        # target_clss = 'aiqiyi'\n",
    "\n",
    "        # ack_pad = False\n",
    "\n",
    "        can_flows = data_len_dir[src_clss]\n",
    "\n",
    "\n",
    "        # org_flow_list_tmp = [flow for flow in can_flows]\n",
    "        # org_flow_list = []\n",
    "        # for item0 in org_flow_list_tmp:\n",
    "        #     org_flow_list.append([item[1] for item in item0])\n",
    "\n",
    "        payloads_blocks_list = []\n",
    "        for flow in can_flows:\n",
    "            #for each flow, first we gather the payload sequence blocks\n",
    "            payloads_blocks = [] #(direction, start_idx, end_idx, flags(A,P), payload_len )\n",
    "            i = 0\n",
    "        #     print(flow)\n",
    "            while i < len(flow)-1:\n",
    "        #         print(flow[i])\n",
    "        #     for i in range(len(flow)):\n",
    "                if flow[i][2] == 0:\n",
    "        #             payloads_blocks.append((flow[i][0],i,i,'A',0))\n",
    "                    i += 1\n",
    "                    continue\n",
    "                start_idx = i\n",
    "                payload_cul = []\n",
    "                end_idx = 0\n",
    "                #find the next opposite pkt\n",
    "                for j in range(i,len(flow)):\n",
    "                    if flow[j][0] != flow[i][0] and flow[j][2] !=0:\n",
    "\n",
    "                        end_idx = j\n",
    "        #                 payload_cul = np.array([item[2] for item in flow[i:j+1]]).sum()\n",
    "        #                 payloads_block.append(flow[i][0],start_idx,end_idx,payload_cul)\n",
    "                        break\n",
    "                    end_idx = j\n",
    "                payload_cul = np.array([item[2] for item in flow[i:j]]).sum()\n",
    "\n",
    "        #         print(i,j)\n",
    "                if end_idx == len(flow) -1:\n",
    "                    payloads_blocks.append((flow[i][0],start_idx,end_idx,payload_cul))\n",
    "                else:\n",
    "                    payloads_blocks.append((flow[i][0],start_idx,end_idx-1,payload_cul))\n",
    "\n",
    "                i = end_idx\n",
    "            payloads_blocks_list.append(payloads_blocks)\n",
    "        #     print(payloads_blocks)\n",
    "        #         break\n",
    "        #     break\n",
    "\n",
    "        #payloads_blocks_list obtained, targeted attacking started\n",
    "        target_trans_array = trans_dir_20[target_clss]\n",
    "        target_adj_len_headers = top20_dir[target_clss]\n",
    "        init_trans_array = np.diagonal(target_trans_array[0])\n",
    "        fund_len = 54\n",
    "\n",
    "        target_handshake_seq = handshake_seq_dir[target_clss]\n",
    "\n",
    "        trans_flow_list = [] # pkt lens seq\n",
    "\n",
    "        for k in range(len(can_flows)):\n",
    "            flow = can_flows[k]\n",
    "            hs_len = flow[0][-5]\n",
    "            payloads_block = payloads_blocks_list[k]\n",
    "            payload_cul = 0\n",
    "            trans_idx = 0\n",
    "            pre_lens_payload = 0\n",
    "            trans_flow = []\n",
    "            #append handshake pkts ([seq],start,end,'A/P')\n",
    "            trans_flow.append(([target_handshake_seq[0]],0,0,'A'))\n",
    "            trans_flow.append((([target_handshake_seq[2]],2,2,'A')))\n",
    "        #     print(payloads_block)\n",
    "\n",
    "        #     print(payloads_block)\n",
    "            #select the initial point for the first block\n",
    "            first_tuple = [(init_trans_array[x], x, target_adj_len_headers[x]) for x in range(len(init_trans_array))] #(count, idx, header)\n",
    "            first_tuple_sorted = sorted(first_tuple, key = lambda x:x[0], reverse=True)\n",
    "\n",
    "            #get the first positive direction block\n",
    "            first_idx = None\n",
    "            for g in range(len(payloads_block)):\n",
    "                if payloads_block[g][0] == 1:\n",
    "                    first_idx = g\n",
    "                    break\n",
    "        #         else:\n",
    "        #             #if the first block is negative direction, append anyway in adj_flow\n",
    "        #             for g1 in range(payloads_block[g][1],payloads_block[g][2]+1):\n",
    "        #                 trans_flow.append(flow[g1][1])\n",
    "            if first_idx == None: #error, append the original flow\n",
    "        #         trans_flow.append([item[1] for item in flow])\n",
    "                print(\"Error, no positive block detected!\")\n",
    "                print(flow[0][-4])             \n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            selected_first_length = None\n",
    "            start_idx = payloads_block[first_idx][1]\n",
    "            end_idx = payloads_block[first_idx][2]\n",
    "            first_seq = []\n",
    "\n",
    "            for lens in first_tuple_sorted:\n",
    "                if payloads_block[first_idx][-1] >= lens[-1]:\n",
    "                    selected_first_length = lens[-1]\n",
    "                    break\n",
    "            if selected_first_length != None:\n",
    "                first_seq.append(fund_len + selected_first_length)\n",
    "        #         trans_flow.append(fund_len)\n",
    "\n",
    "                payload_cul += (payloads_block[first_idx][-1] - selected_first_length)\n",
    "                pre_lens_payload = selected_first_length\n",
    "                trans_idx += 1\n",
    "            else:\n",
    "                first_seq.append(fund_len + payloads_block[first_idx][-1])\n",
    "        #         trans_flow.append(fund_len)\n",
    "\n",
    "                payload_cul += (payloads_block[first_idx][-1] - payloads_block[first_idx][-1])\n",
    "                pre_lens_payload = target_adj_len_headers[np.argmax(init_trans_array)]\n",
    "                trans_idx += 1\n",
    "\n",
    "            # first pkt of the first block has been re-organized, then we need to deal with the margin\n",
    "#             print(trans_idx)\n",
    "            ff_idx = list(target_adj_len_headers).index(pre_lens_payload)\n",
    "            current_trans_array = target_trans_array[1][ff_idx]\n",
    "            while payload_cul != 0:\n",
    "                idx_tmp = list(target_adj_len_headers).index(pre_lens_payload)\n",
    "        #         print(trans_idx,idx_tmp)\n",
    "                #idx exceeds\n",
    "                if trans_idx >= target_trans_array.shape[0]:\n",
    "                    while payload_cul > 1460:\n",
    "                        first_seq.append(1460+fund_len)\n",
    "                        payload_cul -= 1460\n",
    "                    first_seq.append(payload_cul+fund_len)\n",
    "        #             trans_flow.append(fund_len)\n",
    "                    payload_cul = 0\n",
    "                    pre_lens_payload = target_adj_len_headers[np.argmax(current_trans_array)]\n",
    "                    trans_idx += 1\n",
    "                    break\n",
    "\n",
    "                current_trans_array = target_trans_array[1][idx_tmp]\n",
    "#                 print(current_trans_array)\n",
    "                current_tuple = [(current_trans_array[x], x, target_adj_len_headers[x]) for x in range(len(current_trans_array))] #(count, idx, header)\n",
    "                current_tuple_sorted = sorted(current_tuple, key = lambda x:x[0], reverse=True)\n",
    "                \n",
    "                \n",
    "                #select the targeted lens\n",
    "                select_idx = None\n",
    "                for item in current_tuple_sorted:\n",
    "                    if payload_cul >= item[-1]:\n",
    "                        select_idx = item[1]\n",
    "                        break\n",
    "                \n",
    "                if select_idx != None:\n",
    "                    first_seq.append(target_adj_len_headers[select_idx]+fund_len)\n",
    "        #             trans_flow.append(fund_len)\n",
    "\n",
    "                    payload_cul -= target_adj_len_headers[select_idx]\n",
    "                    pre_lens_payload = target_adj_len_headers[select_idx]\n",
    "                    trans_idx =  (trans_idx + 1) % 20\n",
    "#                     print(current_tuple_sorted)\n",
    "#                     print(\"PaCul:\", str(payload_cul), \"idx: \", str(select_idx), \"sel_len: \", str(target_adj_len_headers[select_idx]))\n",
    "#                     print(select_idx)\n",
    "#                     print(target_adj_len_headers[select_idx])\n",
    "                else:# below the least value\n",
    "        #             print(payload_cul)\n",
    "                    first_seq.append(payload_cul+fund_len)\n",
    "        #             trans_flow.append(fund_len)\n",
    "                    payload_cul = 0\n",
    "                    pre_lens_payload = target_adj_len_headers[np.argmax(current_trans_array)]\n",
    "#                     print(pre_lens_payload)\n",
    "                    trans_idx =  (trans_idx + 1) % 20\n",
    "                    \n",
    "\n",
    "            #append the first flow block to list\n",
    "            trans_flow.append((first_seq,start_idx+hs_len,end_idx+hs_len,'P'))\n",
    "\n",
    "            # deal with the following blocks\n",
    "            for i in range(first_idx+1,len(payloads_block)):\n",
    "#                 print(trans_idx)\n",
    "#                 trans_idx = trans_idx % 20\n",
    "                current_seq = []\n",
    "        #         print(current_block)\n",
    "                current_block = payloads_block[i]\n",
    "                start_idx = current_block[1]\n",
    "                end_idx = current_block[2]\n",
    "                # negative direction blocks\n",
    "                if current_block[0] == -1:\n",
    "        #             for g1 in range(current_block[1],current_block[2]+1):\n",
    "        #                 trans_flow.append(flow[g1][1])\n",
    "                    continue\n",
    "\n",
    "                # positive direction block, trans_idx out of consideration\n",
    "                if current_block[0] == 1 and trans_idx >= target_trans_array.shape[0]:\n",
    "                    for pkt in flow[current_block[1]:current_block[2]+1]:\n",
    "                        if pkt[0] == 1:  \n",
    "                            current_seq.append(pkt[2] + fund_len)\n",
    "                    if current_block[-1] == 0:\n",
    "                        trans_flow.append((current_seq,current_block[1]+hs_len,current_block[2]+hs_len,'A'))\n",
    "                    else:\n",
    "                        trans_flow.append((current_seq,current_block[1]+hs_len,current_block[2]+hs_len,'P'))\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # positive direction blocks\n",
    "                if current_block[0] == 1:\n",
    "\n",
    "                    payload_cul = current_block[-1]\n",
    "\n",
    "        #             print(current_block)\n",
    "                    while payload_cul != 0:\n",
    "                        # idx exceeds append randomly\n",
    "                        if trans_idx >=target_trans_array.shape[0]:\n",
    "                            if payload_cul == 0:\n",
    "                                break\n",
    "        #                     print(int(payload_cul/10))\n",
    "                            pkt_number = random.randint(1,math.ceil(payload_cul/10))\n",
    "                            seq = randomSeq(payload_cul, pkt_number)\n",
    "                            for item in seq:\n",
    "                                current_seq.append(item+fund_len)\n",
    "        #                         trans_flow.append(fund_len)\n",
    "                            payload_cul = 0\n",
    "                            break\n",
    "\n",
    "                        # normal idx\n",
    "                        idx_tmp = list(target_adj_len_headers).index(pre_lens_payload)\n",
    "                        current_trans_array = target_trans_array[1][idx_tmp]\n",
    "                        # rank\n",
    "                        current_tuple = [(current_trans_array[x], x, target_adj_len_headers[x]) for x in range(len(current_trans_array))] #(count, idx, header)\n",
    "                        current_tuple_sorted = sorted(current_tuple, key = lambda x:x[0], reverse=True)\n",
    "                        #select the targeted lens\n",
    "                        select_idx = None\n",
    "                        for item in current_tuple_sorted:\n",
    "                            if payload_cul >= item[-1]:\n",
    "                                select_idx = item[1]\n",
    "                                break\n",
    "\n",
    "                        if select_idx != None:\n",
    "                            current_seq.append(target_adj_len_headers[select_idx]+fund_len)\n",
    "        #                     trans_flow.append(fund_len)\n",
    "                            payload_cul -= target_adj_len_headers[select_idx]\n",
    "                            pre_lens_payload = target_adj_len_headers[select_idx]\n",
    "#                             print(pre_lens_payload)\n",
    "                            trans_idx =  (trans_idx + 1) % 20\n",
    "                        else:# below the least value\n",
    "                            current_seq.append(payload_cul+fund_len)\n",
    "        #                     trans_flow.append(fund_len)\n",
    "                            payload_cul = 0\n",
    "                            pre_lens_payload = target_adj_len_headers[np.argmax(current_trans_array)]\n",
    "#                             print(pre_lens_payload)\n",
    "                            trans_idx =  (trans_idx + 1) % 20\n",
    "                    trans_flow.append((current_seq,start_idx+hs_len,end_idx+hs_len,'P'))\n",
    "                    continue\n",
    "\n",
    "            #append save pkts\n",
    "            #locate the fin idx\n",
    "        #     if payloads_block[-1][0] != -1:\n",
    "        #         fin_idx = None\n",
    "        #         for i in range(len(flow)):\n",
    "        #             if 'F' in flow[i][-2]:\n",
    "        #                 fin_idx = i\n",
    "        #                 break\n",
    "        #         if fin_idx != None:\n",
    "        #             for i in range(fin_idx,len(flow)):\n",
    "        #                 trans_flow.append(flow[i][1])\n",
    "        #     print(trans_flow)\n",
    "            trans_flow_list.append((flow[0][-4], trans_flow))\n",
    "#             print(trans_flow)\n",
    "        #     print(trans_flow)\n",
    "        #     print(flow)\n",
    "        #     print(target_adj_len_headers)\n",
    "#             break\n",
    "        \n",
    "        targeted_flows_dir[target_clss] = trans_flow_list\n",
    "#         break\n",
    "#     break\n",
    "    targeted_flows_dir_dir[src_clss] = targeted_flows_dir\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil\n",
    "from scapy.all import *\n",
    "from scapy.utils import PcapReader\n",
    "import random\n",
    "import copy\n",
    "# reverse to the original flows\n",
    "rej = ['FTPS','SFTP']\n",
    "root = 'data'\n",
    "root_dst = 'targeted_data'\n",
    "root_adj_undir = 'data_targeted_undir'\n",
    "root_adj_bidir = 'data_targeted_bidir'\n",
    "\n",
    "root_adj_undir = os.path.join(root_dst,root_adj_undir)\n",
    "root_adj_bidir = os.path.join(root_dst,root_adj_bidir)\n",
    "\n",
    "if not os.path.exists(root_adj_undir):\n",
    "    os.mkdir(root_adj_undir)\n",
    "\n",
    "if not os.path.exists(root_adj_bidir):\n",
    "    os.mkdir(root_adj_bidir)\n",
    "\n",
    "for src_label in list(targeted_flows_dir_dir.keys()):\n",
    "    if src_label in rej:\n",
    "        continue\n",
    "    src_label_path_undir = os.path.join(root_adj_undir, src_label)\n",
    "    src_label_path_bidir = os.path.join(root_adj_bidir, src_label)\n",
    "    \n",
    "    targeted_flows_dir = targeted_flows_dir_dir[src_label]\n",
    "    \n",
    "    for label in list(targeted_flows_dir.keys()):\n",
    "        if label in rej:\n",
    "            continue\n",
    "        dir_path = os.path.join(root,src_label)\n",
    "        dir_path_targeted_undir = os.path.join(src_label_path_undir,label)\n",
    "        dir_path_targeted_bidir = os.path.join(src_label_path_bidir,label)\n",
    "\n",
    "        if not os.path.exists(dir_path_targeted_undir):\n",
    "            os.makedirs(dir_path_targeted_undir)\n",
    "\n",
    "        if not os.path.exists(dir_path_targeted_bidir):\n",
    "            os.makedirs(dir_path_targeted_bidir)\n",
    "\n",
    "\n",
    "        # flow level\n",
    "        if label in ['SFTP','FTPS']:\n",
    "            continue\n",
    "        print(\"***** Reversing\",src_label, \" ==> \" ,label,\" *****\")\n",
    "        for flow_pair in targeted_flows_dir[label]:\n",
    "            double_break = False\n",
    "            file = flow_pair[0]\n",
    "            blocks = flow_pair[1] #tuple ([86, 81, 152, 67], 13, 14, 'P')\n",
    "            if os.path.exists(os.path.join(dir_path_targeted_undir,file)):\n",
    "                continue\n",
    "            try:\n",
    "#             if True:\n",
    "\n",
    "                #open file \n",
    "                org_flow = rdpcap(os.path.join(dir_path,file))\n",
    "\n",
    "                #confirm the src/dst ip\n",
    "                src_ip = None\n",
    "                src_port = None\n",
    "                src_mac = None\n",
    "                dst_ip = None\n",
    "                dst_port = None\n",
    "                dst_mac = None\n",
    "                for ff in org_flow:\n",
    "                    if ff['TCP'].flags == 'S':\n",
    "                        src_ip = ff['IP'].src\n",
    "                        dst_ip = ff['IP'].dst\n",
    "\n",
    "                        src_port = ff['TCP'].sport\n",
    "                        dst_port = ff['TCP'].dport\n",
    "\n",
    "                        src_mac = ff['Ether'].src\n",
    "                        dst_mac = ff['Ether'].dst     \n",
    "                    if ff['TCP'].flags == 'SA':\n",
    "                        src_ip = ff['IP'].dst\n",
    "                        dst_ip = ff['IP'].src\n",
    "\n",
    "                        src_port = ff['TCP'].dport\n",
    "                        dst_port = ff['TCP'].sport\n",
    "\n",
    "                        src_mac = ff['Ether'].dst\n",
    "                        dst_mac = ff['Ether'].src\n",
    "                        break\n",
    "                if src_ip == None:\n",
    "                    print('Error')\n",
    "                    print(file)\n",
    "                    break\n",
    "\n",
    "\n",
    "                #idx reconstruction\n",
    "                recon_blocks = []\n",
    "                for i in range(len(blocks)-1):\n",
    "                    \n",
    "                    current_end_idx = blocks[i][-2]\n",
    "                    next_start_idx = blocks[i+1][1]\n",
    "\n",
    "                    recon_blocks.append((blocks[i],1))\n",
    "\n",
    "                    if current_end_idx + 1 != next_start_idx:\n",
    "                        recon_blocks.append(((current_end_idx+1,next_start_idx-1),-1))\n",
    "                recon_blocks.append((blocks[-1],1))\n",
    "                # deal with the remaining pkts\n",
    "                if blocks[-1][-2] < len(org_flow)-1:\n",
    "                    recon_blocks.append(((blocks[-1][-2]+1,len(org_flow)-1),-1))\n",
    "\n",
    "                #traverse the recon_blocks\n",
    "                # maintain a ack and a seq numbers\n",
    "                idx = recon_blocks[0][1]\n",
    "                ack = org_flow[idx]['TCP'].ack\n",
    "                seq = org_flow[idx]['TCP'].seq\n",
    "\n",
    "                # eastablish a standard pkt\n",
    "                standard_pkt = None\n",
    "                for pkt in org_flow:\n",
    "                    if len(pkt) >=70:\n",
    "                        standard_pkt = pkt\n",
    "                if standard_pkt == None:\n",
    "                    print(\"Standard pkt not found !\")\n",
    "                    print(file)\n",
    "                    double_break = True\n",
    "                    continue\n",
    "\n",
    "\n",
    "                adj_flow_pcap_bidir = []\n",
    "                adj_flow_pcap_undir = []\n",
    "\n",
    "                for block_pair in recon_blocks:\n",
    "                    if double_break == True:\n",
    "                        break\n",
    "                    block = block_pair[0]\n",
    "\n",
    "                    #extract ack and seq number of the first pkt of each block\n",
    "                    idx = block[1]\n",
    "                    ack = org_flow[idx]['TCP'].ack\n",
    "                    seq = org_flow[idx]['TCP'].seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # positive direction blocks\n",
    "                    if block_pair[-1] == 1:\n",
    "                        #[start,end)\n",
    "                        start_idx = block[1]\n",
    "                        end_idx = block[2] + 1\n",
    "\n",
    "                        if block[-1] == 'A': #block without payload\n",
    "                            for i in range(start_idx,end_idx):\n",
    "                                tmp_pkt = copy.deepcopy(org_flow[i])\n",
    "                                adj_flow_pcap_bidir.append(tmp_pkt)\n",
    "                                adj_flow_pcap_undir.append(tmp_pkt)\n",
    "\n",
    "                            continue\n",
    "                        if block[-1] == 'P': #block with payload\n",
    "                            # cumulate payload from one block\n",
    "                            payload_cul = bytearray()\n",
    "                            for i in range(start_idx,end_idx):\n",
    "                                if org_flow[i].haslayer(\"Raw\"):\n",
    "                                    payload_cul += bytearray(bytes(org_flow[i]['Raw']))\n",
    "                            adj_seq = block[0]\n",
    "                            #check out if the culmulate payload generated sequence equals to the original payload length\n",
    "                            if np.array(adj_seq).sum() - np.array(adj_seq).shape[0]*54 != len(payload_cul):\n",
    "                                print(\"Error! payloads length unequal!\")\n",
    "                                print(file)\n",
    "                                print(block)\n",
    "#                                 break\n",
    "                            # construct the fundamental pkt\n",
    "                            pkt_recon_fund = copy.deepcopy(standard_pkt)\n",
    "                            pkt_recon_fund[\"Ether\"].src = src_mac\n",
    "                            pkt_recon_fund[\"Ether\"].dst = dst_mac\n",
    "                            pkt_recon_fund[\"IP\"].dst = dst_ip\n",
    "                            pkt_recon_fund[\"IP\"].src = src_ip\n",
    "                            pkt_recon_fund[\"TCP\"].dport = dst_port\n",
    "                            pkt_recon_fund[\"TCP\"].sport = src_port\n",
    "                            # ack = ack[0]\n",
    "                            pkt_recon_fund[\"TCP\"].ack = ack\n",
    "\n",
    "                            #random time seq\n",
    "                            time_start = org_flow[block[1]].time\n",
    "                            time_end = org_flow[block[2]].time\n",
    "                            duration = time_end - time_start\n",
    "        #                     print(duration)\n",
    "                            time_seq = [duration/(len(adj_seq)*1.0) for x in range(len(adj_seq))]\n",
    "                            if len(time_seq) > 3:\n",
    "\n",
    "                                time_seq[1] += time_seq[0]\n",
    "                                time_seq[0] = 0\n",
    "                                time_seq[-2] += time_seq[-1]\n",
    "                                time_seq[-1] = 0\n",
    "\n",
    "                            # construct each pkt\n",
    "                            payload_used_cul = 0\n",
    "                            for i in range(len(adj_seq)):\n",
    "                                gen_pkt_len = adj_seq[i]\n",
    "                                gen_pkt_time = time_seq[i]\n",
    "                                payload_used = gen_pkt_len - fund_len\n",
    "\n",
    "                                # copy from fundamental pkt\n",
    "                                pkt_recon = copy.deepcopy(pkt_recon_fund)\n",
    "                                # seq[i] = seq[0] + payload_used_cul\n",
    "                                pkt_recon[\"TCP\"].seq = seq + payload_used_cul\n",
    "                                pkt_recon.time = time_start + gen_pkt_time\n",
    "                                time_start += gen_pkt_time\n",
    "                                pkt_recon[\"TCP\"].flags = \"PA\" #PSH ACK\n",
    "\n",
    "                                # concat payload payload_cul\n",
    "                                im_py = payload_cul[payload_used_cul:payload_used_cul+payload_used]\n",
    "        #                         print(repr(pkt_recon))\n",
    "                                pkt_recon['Raw'] = Packet(im_py)\n",
    "\n",
    "                                payload_used_cul += payload_used\n",
    "\n",
    "                                #convert pkt to btye flow\n",
    "                                pkt_recon.len = len(pkt_recon)\n",
    "                                pkt_recon['IP'].id += i\n",
    "                                pkt_recon['IP'].len = (len(pkt_recon) -14)\n",
    "        #                         print(len(pkt_recon))\n",
    "                                #append to new flow\n",
    "                                adj_flow_pcap_bidir.append(pkt_recon)\n",
    "                                adj_flow_pcap_undir.append(pkt_recon)\n",
    "\n",
    "\n",
    "                    # negative direction blocks\n",
    "                    if block_pair[-1] == -1:\n",
    "                        start_idx = block[0]\n",
    "                        end_idx = block[1] + 1\n",
    "                        for i in range(start_idx,end_idx):\n",
    "                            adj_flow_pcap_bidir.append(org_flow[i])\n",
    "                        #[start,end)\n",
    "            except:\n",
    "                print(\"Error\")\n",
    "                print(file)\n",
    "                continue\n",
    "            if double_break:\n",
    "                continue\n",
    "            output_path_targeted_undir = os.path.join(dir_path_targeted_undir,file)\n",
    "            output_path_targeted_bidir = os.path.join(dir_path_targeted_bidir,file)\n",
    "\n",
    "\n",
    "\n",
    "            #write the undirectional targeted flow\n",
    "            writer = PcapWriter(output_path_targeted_undir)\n",
    "            for pkt in adj_flow_pcap_undir:\n",
    "                writer.write(pkt)\n",
    "            writer.close()\n",
    "\n",
    "            #write the bidirectional targeted flow\n",
    "#             writer = PcapWriter(output_path_targeted_bidir)\n",
    "#             for pkt in adj_flow_pcap_bidir:\n",
    "#                 writer.write(pkt)\n",
    "#             writer.close()\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         org_undir_flow = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
