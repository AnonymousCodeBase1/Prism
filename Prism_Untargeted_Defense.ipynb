{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class save_data(object):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "with open(\"merge_data.pkl\",\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "#dir, pktlen, payload len, flag, label\n",
    "data_dir = {}\n",
    "\n",
    "for item in data.data:\n",
    "\n",
    "    label = item[0][-1]\n",
    "    if label not in list(data_dir.keys()):\n",
    "        data_dir[label] ={}\n",
    "    \n",
    "    for pkt in item:\n",
    "        if pkt[0] == -1:\n",
    "            continue\n",
    "        lens = pkt[2] \n",
    "        if lens == 0:\n",
    "            continue\n",
    "        \n",
    "        if lens not in list(data_dir[label].keys()):\n",
    "            data_dir[label][lens] = 0\n",
    "        data_dir[label][lens] += 1\n",
    "        \n",
    "data_dir_tuple = {}\n",
    "\n",
    "for label in data_dir.keys():\n",
    "    data_dir_tuple[label] = sorted(data_dir[label].items(),key=lambda item:item[1],reverse = True)\n",
    "    \n",
    "top20_dir = {}\n",
    "for label in data_dir_tuple.keys():\n",
    "    top20_dir[label] = sorted([item[0] for item in data_dir_tuple[label][:20]])\n",
    "    \n",
    "data_dir_adj = {}\n",
    "for flow in data.data:\n",
    "    label = flow[0][-1]\n",
    "    \n",
    "    if label not in list(data_dir_adj.keys()):\n",
    "        data_dir_adj[label] = []\n",
    "    tmp_flow = [item[1] for item in flow]\n",
    "    adj_flow = []\n",
    "    for item in tmp_flow:\n",
    "        idx_val = [abs(item-cand) for cand in top20_dir[label]]\n",
    "        adj_flow.append(top20_dir[label][np.argmin(np.array(idx_val))])\n",
    "    data_dir_adj[label].append(adj_flow)\n",
    "\n",
    "\n",
    "#define a trans seq of 20  top20_dir data_dir_adj\n",
    "trans_dir_20 = {}\n",
    "seq_len = 20\n",
    "for label in data_dir_adj:\n",
    "    if label not in list(trans_dir_20.keys()):\n",
    "        trans_dir_20[label] = np.zeros((seq_len,20,20))\n",
    "    top_20 = [item for item in top20_dir[label] ]\n",
    "    #initial point\n",
    "    for flow in data_dir_adj[label]:\n",
    "#         for i in range(seq_len):\n",
    "#             if i >len(flow)-1:\n",
    "#                 break\n",
    "        idx = top_20.index(flow[0])\n",
    "        trans_dir_20[label][0][idx][idx] += 1\n",
    "    \n",
    "#     trans_dir_20[label][0] = trans_dir_20[label][0]/np.diagonal(trans_dir_20[label][0]).sum()\n",
    "    #trans matrix\n",
    "    \n",
    "    for epoch in range(1,seq_len):\n",
    "        for flow in data_dir_adj[label]:\n",
    "            if epoch > len(flow)-1:\n",
    "                continue\n",
    "            idx_pre = top_20.index(flow[epoch-1])\n",
    "            idx = top_20.index(flow[epoch])\n",
    "            trans_dir_20[label][epoch][idx_pre][idx] += 1\n",
    "        \n",
    "        \n",
    "#dir, pktlen, payload len, flag, label\n",
    "data_len_dir = {}\n",
    "\n",
    "for item in data.data:\n",
    "\n",
    "    label = item[0][-1]\n",
    "    if label not in list(data_len_dir.keys()):\n",
    "        data_len_dir[label] =[]\n",
    "    \n",
    "    data_len_dir[label].append(item)\n",
    "\n",
    "    \n",
    "def randomSeq(maxValue, num):\n",
    "\n",
    "    maxValue = int(maxValue)\n",
    "    suiji_ser = random.sample(range(1,maxValue), k=num-1)\n",
    "    suiji_ser.append(0)  \n",
    "    suiji_ser.append(maxValue)\n",
    "    suiji_ser = sorted(suiji_ser)\n",
    "    per_all_persons = [ suiji_ser[i]-suiji_ser[i-1] for i in range(1, len(suiji_ser)) ] \n",
    "    \n",
    "    return per_all_persons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handshake seq counting\n",
    "handshake_seq_dir = {}\n",
    "for label in list(data_len_dir.keys()):\n",
    "    tmp = {}\n",
    "    for flow in data_len_dir[label]:\n",
    "#         print(item)\n",
    "        if flow[0][-3] not in list(tmp.keys()):\n",
    "            tmp[flow[0][-3]] = 0\n",
    "        tmp[flow[0][-3]] += 1\n",
    "    \n",
    "    tmp = sorted(tmp.items(), key=lambda d:d[1], reverse = True)\n",
    "    handshake_seq_dir[label] = tmp[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#untargeted attacking\n",
    "import random\n",
    "\n",
    "untargeted_flows_dir = {}\n",
    "for src_clss in list(handshake_seq_dir.keys()):\n",
    "    print(\"***** Refracting \", src_clss, \" *****\")\n",
    "\n",
    "    can_flows = data_len_dir[src_clss]\n",
    "    fund_len = 54\n",
    "    \n",
    "    adj_flows_untargetd = [handshake_seq_dir[src_clss][0],handshake_seq_dir[src_clss][2]]\n",
    "\n",
    "#     org_flows_tmp = [flow for flow in can_flows]\n",
    "#     org_flows = []\n",
    "#     for item0 in org_flows_tmp:\n",
    "#         org_flows.append([item[1] for item in item0])\n",
    "\n",
    "    for flow in can_flows:\n",
    "        tmp_flow = []\n",
    "        \n",
    "        payload_cul = 0\n",
    "\n",
    "        for k in range(len(flow)):\n",
    "            pkt = flow[k]\n",
    "            #negative direction pkt\n",
    "            if pkt[0] == -1:\n",
    "                #without payload\n",
    "                if pkt[2] == 0:\n",
    "#                     tmp_flow.append(pkt[1])\n",
    "                    continue\n",
    "                #with payload\n",
    "                if pkt[2] != 0:\n",
    "#                     tmp_flow.append(pkt[1])\n",
    "                    continue\n",
    "            #positive direction pkt\n",
    "            if pkt[0] ==1:\n",
    "                #without payload\n",
    "                if pkt[2] == 0:\n",
    "                    tmp_flow.append(pkt[1])\n",
    "                    continue\n",
    "                #with payload\n",
    "                if pkt[2] != 0:\n",
    "                    payload = pkt[2]\n",
    "\n",
    "                    if payload <=20:\n",
    "                        tmp_flow.append(pkt[1])\n",
    "                        continue\n",
    "                    pkt_number = random.randint(1,int(payload/10))\n",
    "                    seq = randomSeq(payload, pkt_number)\n",
    "                    for i in range(len(seq)-1):\n",
    "                        tmp_flow.append(fund_len+seq[i])\n",
    "#                         tmp_flow.append(fund_len)\n",
    "\n",
    "                    tmp_flow.append(fund_len+seq[-1])\n",
    "\n",
    "                    continue\n",
    "\n",
    "        adj_flows_untargetd.append(tmp_flow)\n",
    "    untargeted_flows_dir[src_clss] = adj_flows_untargetd\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enhanced Untargeted attack, two items should be provided : trans_array => trans_dir_20, adjusted length(headers) => top20_dir\n",
    "import random,math\n",
    "\n",
    "untargeted_flows_dir = {}\n",
    "for src_clss in list(handshake_seq_dir.keys()):\n",
    "    print(\"***** Refracting \", src_clss, \" *****\")\n",
    "    # ack_pad = False\n",
    "    can_flows = data_len_dir[src_clss]\n",
    "\n",
    "\n",
    "    payloads_blocks_list = []\n",
    "    for flow in can_flows:\n",
    "        #for each flow, first we gather the payload sequence blocks\n",
    "        payloads_blocks = [] #(direction, start_idx, end_idx, flags(A,P), payload_len )\n",
    "        i = 0\n",
    "    #     print(flow)\n",
    "        while i < len(flow)-1:\n",
    "    #         print(flow[i])\n",
    "    #     for i in range(len(flow)):\n",
    "            if flow[i][2] == 0:\n",
    "    #             payloads_blocks.append((flow[i][0],i,i,'A',0))\n",
    "                i += 1\n",
    "                continue\n",
    "            start_idx = i\n",
    "            payload_cul = []\n",
    "            end_idx = 0\n",
    "            #find the next opposite pkt\n",
    "            for j in range(i,len(flow)):\n",
    "                if flow[j][0] != flow[i][0] and flow[j][2] !=0:\n",
    "\n",
    "                    end_idx = j\n",
    "    #                 payload_cul = np.array([item[2] for item in flow[i:j+1]]).sum()\n",
    "    #                 payloads_block.append(flow[i][0],start_idx,end_idx,payload_cul)\n",
    "                    break\n",
    "                end_idx = j\n",
    "            payload_cul = np.array([item[2] for item in flow[i:j]]).sum()\n",
    "\n",
    "    #         print(i,j)\n",
    "            if end_idx == len(flow) -1:\n",
    "                payloads_blocks.append((flow[i][0],start_idx,end_idx,payload_cul))\n",
    "            else:\n",
    "                payloads_blocks.append((flow[i][0],start_idx,end_idx-1,payload_cul))\n",
    "\n",
    "            i = end_idx\n",
    "        payloads_blocks_list.append(payloads_blocks)\n",
    "    #     print(payloads_blocks)\n",
    "    #         break\n",
    "    #     break\n",
    "\n",
    "    #payloads_blocks_list obtained\n",
    "    fund_len = 54\n",
    "\n",
    "    trans_flow_list = [] # pkt lens seq\n",
    "\n",
    "    for k in range(len(can_flows)):\n",
    "        flow = can_flows[k]\n",
    "        hs_len = flow[0][-5]\n",
    "        payloads_block = payloads_blocks_list[k]\n",
    "        payload_cul = 0\n",
    "        trans_idx = 0\n",
    "        pre_lens_payload = 0\n",
    "        trans_flow = []\n",
    "        #append handshake pkts\n",
    "        trans_flow.append(([66],0,0,'A'))\n",
    "        trans_flow.append(([54],2,2,'A'))\n",
    "    #     trans_flow.append(60)\n",
    "    #     trans_flow.append((54,2,'A'))\n",
    "    #     print(payloads_block)\n",
    "\n",
    "    #     print(payloads_block)\n",
    "        #select the initial point for the first block\n",
    "\n",
    "        #get the first positive direction block\n",
    "        first_idx = None\n",
    "        for g in range(len(payloads_block)):\n",
    "            if payloads_block[g][0] == 1:\n",
    "                first_idx = g\n",
    "                break\n",
    "#             else:\n",
    "#                 start_idx = payloads_block[g][1]\n",
    "#                 end_idx = payloads_block[g][2]\n",
    "#                 for kk in range(start_idx,end_idx+1):\n",
    "#                     #append pos directional pkt only\n",
    "#                     if flow[kk][0] == 1 and flow[kk][2] == 0:\n",
    "#                         trans_flow.append(([flow[kk][1]],kk+hs_len,kk+hs_len,'A'))\n",
    "\n",
    "\n",
    "        if first_idx == None: #error\n",
    "            \n",
    "            print(\"Error\")\n",
    "            print(payloads_block)\n",
    "            continue\n",
    "\n",
    "        for block in payloads_block[first_idx:]:\n",
    "            if block[0] == -1:\n",
    "#                 start_idx = block[1]\n",
    "#                 end_idx = block[2]\n",
    "#                 for kk in range(start_idx,end_idx+1):\n",
    "#                     #append pos directional pkt only\n",
    "#                     if flow[kk][0] == 1 and flow[kk][2] == 0:\n",
    "#                         trans_flow.append(([flow[kk][1]],kk+hs_len,kk+hs_len,'A'))\n",
    "                continue\n",
    "\n",
    "            start_idx = block[1]\n",
    "            end_idx = block[2]\n",
    "\n",
    "            payload = block[-1]\n",
    "            if payload == 0:\n",
    "                continue\n",
    "\n",
    "            #random seq\n",
    "            if payload <=20:\n",
    "                trans_flow.append(([payload + fund_len],start_idx+hs_len,end_idx+hs_len,'P'))\n",
    "                continue\n",
    "            pkt_number = random.randint(1,int(payload/10))\n",
    "            seq = randomSeq(payload, pkt_number)\n",
    "            seq = [item + fund_len for item in seq]\n",
    "            seq.append(fund_len)\n",
    "            \n",
    "            trans_flow.append((seq,start_idx+hs_len,end_idx+hs_len,'P'))\n",
    "        trans_flow_list.append((flow[0][-4], trans_flow))\n",
    "    untargeted_flows_dir[src_clss] = trans_flow_list  \n",
    "                    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil\n",
    "from scapy.all import *\n",
    "from scapy.utils import PcapReader\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse to the original flows\n",
    "\n",
    "root = 'data'\n",
    "root_adj_undir = 'data_untargeted_undir'\n",
    "root_adj_bidir = 'data_untargeted_bidir'\n",
    "root_undir_org = 'data_flow_org'\n",
    "\n",
    "if not os.path.exists(root_adj_undir):\n",
    "    os.mkdir(root_adj_undir)\n",
    "if not os.path.exists(root_undir_org):\n",
    "    os.mkdir(root_undir_org)\n",
    "if not os.path.exists(root_adj_bidir):\n",
    "    os.mkdir(root_adj_bidir)\n",
    "\n",
    "\n",
    "for label in list(untargeted_flows_dir.keys()):\n",
    "    dir_path = os.path.join(root,label)\n",
    "    dir_path_untargeted_undir = os.path.join(root_adj_undir,label)\n",
    "    dir_path_untargeted_bidir = os.path.join(root_adj_bidir,label)\n",
    "    dir_path_undir_org = os.path.join(root_undir_org,label)\n",
    "    if not os.path.exists(dir_path_untargeted_undir):\n",
    "        os.mkdir(dir_path_untargeted_undir)\n",
    "    if not os.path.exists(dir_path_untargeted_bidir):\n",
    "        os.mkdir(dir_path_untargeted_bidir)\n",
    "    if not os.path.exists(dir_path_undir_org):\n",
    "        os.mkdir(dir_path_undir_org)\n",
    "    \n",
    "    # flow level\n",
    "    if label in ['SFTP','FTPS']:\n",
    "        continue\n",
    "    print(\"***** Reversing\",label,\" *****\")\n",
    "    for flow_pair in untargeted_flows_dir[label]:\n",
    "        file = flow_pair[0]\n",
    "        blocks = flow_pair[1] #tuple ([86, 81, 152, 67], 13, 14, 'P')\n",
    "        if os.path.exists(os.path.join(dir_path_untargeted_bidir,file)):\n",
    "            continue\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            #open file \n",
    "            org_flow = rdpcap(os.path.join(dir_path,file))\n",
    "            \n",
    "            #confirm the src/dst ip\n",
    "            src_ip = None\n",
    "            src_port = None\n",
    "            src_mac = None\n",
    "            dst_ip = None\n",
    "            dst_port = None\n",
    "            dst_mac = None\n",
    "            for ff in org_flow:\n",
    "                if ff['TCP'].flags == 'S':\n",
    "                    src_ip = ff['IP'].src\n",
    "                    dst_ip = ff['IP'].dst\n",
    "\n",
    "                    src_port = ff['TCP'].sport\n",
    "                    dst_port = ff['TCP'].dport\n",
    "\n",
    "                    src_mac = ff['Ether'].src\n",
    "                    dst_mac = ff['Ether'].dst     \n",
    "                if ff['TCP'].flags == 'SA':\n",
    "                    src_ip = ff['IP'].dst\n",
    "                    dst_ip = ff['IP'].src\n",
    "\n",
    "                    src_port = ff['TCP'].dport\n",
    "                    dst_port = ff['TCP'].sport\n",
    "\n",
    "                    src_mac = ff['Ether'].dst\n",
    "                    dst_mac = ff['Ether'].src\n",
    "                    break\n",
    "            if src_ip == None:\n",
    "                print('Error')\n",
    "                print(file)\n",
    "                break\n",
    "\n",
    "\n",
    "            #idx reconstruction\n",
    "            recon_blocks = []\n",
    "            for i in range(len(blocks)-1):\n",
    "                current_end_idx = blocks[i][-2]\n",
    "                next_start_idx = blocks[i+1][1]\n",
    "\n",
    "                recon_blocks.append((blocks[i],1))\n",
    "\n",
    "                if current_end_idx + 1 != next_start_idx:\n",
    "                    recon_blocks.append(((current_end_idx+1,next_start_idx-1),-1))\n",
    "            recon_blocks.append((blocks[-1],1))\n",
    "            # deal with the remaining pkts\n",
    "            if blocks[-1][-2] < len(org_flow)-1:\n",
    "                recon_blocks.append(((blocks[-1][-2]+1,len(org_flow)-1),-1))\n",
    "\n",
    "            #traverse the recon_blocks\n",
    "            # maintain a ack and a seq numbers\n",
    "            idx = recon_blocks[0][1]\n",
    "            ack = org_flow[idx]['TCP'].ack\n",
    "            seq = org_flow[idx]['TCP'].seq\n",
    "\n",
    "            # eastablish a standard pkt\n",
    "            standard_pkt = None\n",
    "            for pkt in org_flow:\n",
    "                if len(pkt) >=70:\n",
    "                    standard_pkt = pkt\n",
    "            if standard_pkt == None:\n",
    "                print(\"Standard pkt not found !\")\n",
    "                print(file)\n",
    "                continue\n",
    "\n",
    "\n",
    "            adj_flow_pcap_bidir = []\n",
    "            adj_flow_pcap_undir = []\n",
    "\n",
    "            for block_pair in recon_blocks:\n",
    "                block = block_pair[0]\n",
    "\n",
    "                #extract ack and seq number of the first pkt of each block\n",
    "                idx = block[1]\n",
    "                ack = org_flow[idx]['TCP'].ack\n",
    "                seq = org_flow[idx]['TCP'].seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # positive direction blocks\n",
    "                if block_pair[-1] == 1:\n",
    "                    #[start,end)\n",
    "                    start_idx = block[1]\n",
    "                    end_idx = block[2] + 1\n",
    "\n",
    "                    if block[-1] == 'A': #block without payload\n",
    "                        for i in range(start_idx,end_idx):\n",
    "                            tmp_pkt = copy.deepcopy(org_flow[i])\n",
    "                            adj_flow_pcap_bidir.append(tmp_pkt)\n",
    "                            adj_flow_pcap_undir.append(tmp_pkt)\n",
    "\n",
    "                        continue\n",
    "                    if block[-1] == 'P': #block with payload\n",
    "                        # cumulate payload from one block\n",
    "                        payload_cul = bytearray()\n",
    "                        for i in range(start_idx,end_idx):\n",
    "                            if org_flow[i].haslayer(\"Raw\"):\n",
    "                                payload_cul += bytearray(bytes(org_flow[i]['Raw']))\n",
    "                        adj_seq = block[0]\n",
    "                        #check out if the culmulate payload generated sequence equals to the original payload length\n",
    "                        if np.array(adj_seq).sum() - np.array(adj_seq).shape[0]*54 != len(payload_cul):\n",
    "                            print(\"Error! payloads length unequal!\")\n",
    "                            print(file)\n",
    "                            break\n",
    "                        # construct the fundamental pkt\n",
    "                        pkt_recon_fund = copy.deepcopy(standard_pkt)\n",
    "                        pkt_recon_fund[\"Ether\"].src = src_mac\n",
    "                        pkt_recon_fund[\"Ether\"].dst = dst_mac\n",
    "                        pkt_recon_fund[\"IP\"].dst = dst_ip\n",
    "                        pkt_recon_fund[\"IP\"].src = src_ip\n",
    "                        pkt_recon_fund[\"TCP\"].dport = dst_port\n",
    "                        pkt_recon_fund[\"TCP\"].sport = src_port\n",
    "                        # ack = ack[0]\n",
    "                        pkt_recon_fund[\"TCP\"].ack = ack\n",
    "\n",
    "                        #random time seq\n",
    "                        time_start = org_flow[block[1]].time\n",
    "                        time_end = org_flow[block[2]].time\n",
    "                        duration = time_end - time_start\n",
    "    #                     print(duration)\n",
    "                        time_seq = [duration/(len(adj_seq)*1.0) for x in range(len(adj_seq))]\n",
    "                        if len(time_seq) > 3:\n",
    "\n",
    "                            time_seq[1] += time_seq[0]\n",
    "                            time_seq[0] = 0\n",
    "                            time_seq[-2] += time_seq[-1]\n",
    "                            time_seq[-1] = 0\n",
    "\n",
    "                        # construct each pkt\n",
    "                        payload_used_cul = 0\n",
    "                        for i in range(len(adj_seq)):\n",
    "                            gen_pkt_len = adj_seq[i]\n",
    "                            gen_pkt_time = time_seq[i]\n",
    "                            payload_used = gen_pkt_len - fund_len\n",
    "\n",
    "                            # copy from fundamental pkt\n",
    "                            pkt_recon = copy.deepcopy(pkt_recon_fund)\n",
    "                            # seq[i] = seq[0] + payload_used_cul\n",
    "                            pkt_recon[\"TCP\"].seq = seq + payload_used_cul\n",
    "                            pkt_recon.time = time_start + gen_pkt_time\n",
    "                            time_start += gen_pkt_time\n",
    "                            pkt_recon[\"TCP\"].flags = \"PA\" #PSH ACK\n",
    "\n",
    "                            # concat payload payload_cul\n",
    "                            im_py = payload_cul[payload_used_cul:payload_used_cul+payload_used]\n",
    "    #                         print(repr(pkt_recon))\n",
    "                            pkt_recon['Raw'] = Packet(im_py)\n",
    "\n",
    "                            payload_used_cul += payload_used\n",
    "\n",
    "                            #convert pkt to btye flow\n",
    "                            pkt_recon.len = len(pkt_recon)\n",
    "                            pkt_recon['IP'].id += i\n",
    "                            pkt_recon['IP'].len = (len(pkt_recon) -14)\n",
    "    #                         print(len(pkt_recon))\n",
    "                            #append to new flow\n",
    "                            adj_flow_pcap_bidir.append(pkt_recon)\n",
    "                            adj_flow_pcap_undir.append(pkt_recon)\n",
    "\n",
    "\n",
    "                # negative direction blocks\n",
    "                if block_pair[-1] == -1:\n",
    "                    start_idx = block[0]\n",
    "                    end_idx = block[1] + 1\n",
    "                    for i in range(start_idx,end_idx):\n",
    "                        adj_flow_pcap_bidir.append(org_flow[i])\n",
    "                    #[start,end)\n",
    "        except:\n",
    "            print(\"Error\")\n",
    "            print(file)\n",
    "            continue\n",
    "        output_path_undir_org = os.path.join(dir_path_undir_org, file)\n",
    "        output_path_untargeted_undir = os.path.join(dir_path_untargeted_undir,file)\n",
    "        output_path_untargeted_bidir = os.path.join(dir_path_untargeted_bidir,file)\n",
    "        \n",
    "        # org_flow undirected\n",
    "        org_flow_undir = []\n",
    "        for pkt in org_flow:\n",
    "            if pkt['IP'].src == src_ip:\n",
    "                org_flow_undir.append(pkt)\n",
    "        \n",
    "        #write the org undir file\n",
    "        if not os.path.exists(output_path_undir_org):\n",
    "            \n",
    "            writer = PcapWriter(output_path_undir_org)\n",
    "            for pkt in org_flow_undir:\n",
    "                writer.write(pkt)\n",
    "            writer.close()\n",
    "        \n",
    "        #write the undirectional untargeted flow\n",
    "        writer = PcapWriter(output_path_untargeted_undir)\n",
    "        for pkt in adj_flow_pcap_undir:\n",
    "            writer.write(pkt)\n",
    "        writer.close()\n",
    "        \n",
    "        #write the bidirectional untargeted flow\n",
    "        writer = PcapWriter(output_path_untargeted_bidir)\n",
    "        for pkt in adj_flow_pcap_bidir:\n",
    "            writer.write(pkt)\n",
    "        writer.close()\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         org_undir_flow = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
